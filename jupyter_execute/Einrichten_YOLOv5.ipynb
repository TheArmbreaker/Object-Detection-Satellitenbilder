{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtStaZShDeVZ"
   },
   "source": [
    "# YOLOv5 - Grundlagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jegGWSH-Dxux"
   },
   "source": [
    "## Object Detection vs Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_YXDU0D6toz"
   },
   "source": [
    "Die Objekterkennung unterscheidet sich von der Bilderkennung. F√ºr das Projektziel ist die Bilderkennung ungeeignet, weil sie nur ein Objekt erkennen und dieses maximal lokalisieren kann.\n",
    "\n",
    "Stattdessen wird ein Methode ben√∂tigt, welches multiple Objekte erkennt und klassifiziert. F√ºr diese Anwendung gibt es ebenfalls unterschiedliche M√∂glichkeiten wie RCNN oder YOLO. F√ºr diese Arbeit wurde YOLOv5 gew√§hlt, weil dieses verbreitet ist und zum Abschluss des Deployments ein Pandas-Dataframe f√ºr den Use Case erm√∂glicht.\n",
    "\n",
    "Das nachstehende Bild stellt die Unterschiede zwischen Methoden zur Bildklassifikation dar. F√ºr diese Arbeit wird eine Object Detection f√ºr multiple Objects gebraucht, welche im dritten Bild von Links dargestellt wird.\n",
    "\n",
    "```{figure} /nb_images/mil_equip/hund_katze_maus.png\n",
    "---\n",
    "name: Methoden_Bildklassifikation\n",
    "---\n",
    "Methoden zur Bildklassifikation {cite:p}`SAHI_medium`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0mtj-tQxo7b"
   },
   "source": [
    "## Modelle / Vortrainierte Gewichte\n",
    "\n",
    "YOLOv5 bietet verschiedene Grundmodelle f√ºr Deep Learning Neural Networks. Diese Modelle werden in einer YAML-Datei bereitgestellt und k√∂nnen √ºber den --cfg Parameter aufgerufen werden.\n",
    "\n",
    "Die Modelle bestehen aus Backbone und Head. Die Fachliteratur unterscheidet zus√§tzlich einen Neck, welcher in der YAML-Datei bei YOLOv5 im Head dargestellt wird. Der Backbone kann als Feature Extractor und der Head als Classifier verstanden werden.\n",
    "\n",
    "Alternativ zur config werden pretrained Weights bereitgestellt, welche beispielsweise auf dem COCO Datensatz trainiert wurden.\n",
    "\n",
    "\n",
    "```{figure} nb_images/mil_equip/model_comparison.png\n",
    "---\n",
    "name: YOLOv5_Modelle\n",
    "---\n",
    "YOLOv5-Modelle {cite:p}`YOLOv5_TrainCD`\n",
    "```\n",
    "\n",
    "In der nachfolgenden YAML-Datei des S-Modells werden folgende Kategorien abgebildet. Die Struktur ist f√ºr alle Modelle identisch.\n",
    "\n",
    "* nc\n",
    "* depth_multiple\n",
    "* width_multiple\n",
    "* anchors\n",
    "\n",
    "Es ist zu erkennen, dass Backbone und Head unterschiedliche Layer enthalten. Ferner werden Standardwerte f√ºr Anchorboxen gesetzt und Grenzen f√ºr Tiefe / Weite des Netz.\n",
    "\n",
    "Der Parameter nc kann in der Datei angepasst werden oder wird √ºber die Anzahl der Labels automatisch korrigiert.\n",
    "\n",
    "Die Parameter depth_multiple und width_multiple ver√§ndern die Tiefe und Breite des Modells. Diese Werte werden verwendet um einen Unterschied zwischen den Modellen zu erzeugen. D.h. alle Modelle verwenden die gleiche Layer-Architektur in unterschiedlicher Breite und Tiefe.\n",
    "\n",
    "Anchors meint Listen von Anchor-Boxen, welche zur Prediction der Bounding Box ben√∂tigt werden. In YOLOv5 werden die Anchor-Boxen mit automatischer k-means Analyse vom Traininsdaten-Set erlernt, weshalb die manuelle Anpassung optional ist. Generell gilt, dass diese Anchor-Boxen so gew√§hlt werden m√ºssen, dass sie zu den Daten passen. Beispielsweise wird f√ºr ein schmalles l√§ngliches Objekt eine andere Box ben√∂tigt als f√ºr ein gestauchtes breites Objekt.\n",
    "\n",
    "\n",
    "```\n",
    "# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n",
    "\n",
    "# Parameters\n",
    "nc: 80  # number of classes\n",
    "depth_multiple: 0.33  # model depth multiple\n",
    "width_multiple: 0.50  # layer channel multiple\n",
    "anchors:\n",
    "  - [10,13, 16,30, 33,23]  # P3/8\n",
    "  - [30,61, 62,45, 59,119]  # P4/16\n",
    "  - [116,90, 156,198, 373,326]  # P5/32\n",
    "\n",
    "# YOLOv5 v6.0 backbone\n",
    "backbone:\n",
    "  # [from, number, module, args]\n",
    "  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2\n",
    "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
    "   [-1, 3, C3, [128]],\n",
    "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
    "   [-1, 6, C3, [256]],\n",
    "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
    "   [-1, 9, C3, [512]],\n",
    "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
    "   [-1, 3, C3, [1024]],\n",
    "   [-1, 1, SPPF, [1024, 5]],  # 9\n",
    "  ]\n",
    "\n",
    "# YOLOv5 v6.0 head\n",
    "head:\n",
    "  [[-1, 1, Conv, [512, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
    "   [-1, 3, C3, [512, False]],  # 13\n",
    "\n",
    "   [-1, 1, Conv, [256, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
    "   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n",
    "\n",
    "   [-1, 1, Conv, [256, 3, 2]],\n",
    "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
    "   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n",
    "\n",
    "   [-1, 1, Conv, [512, 3, 2]],\n",
    "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
    "   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n",
    "\n",
    "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
    "  ]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBukyLCOD1c9"
   },
   "source": [
    "## Modell-Architektur\n",
    "\n",
    "### Layer\n",
    "\n",
    "In der oben gezeigten Modell-YAML-Datei werden folgende Layer verwendet.\n",
    "\n",
    "* Convolution\n",
    "* C3\n",
    "* SPPF\n",
    "* Upsample\n",
    "* Concat\n",
    "\n",
    "Diese beziehen sich auf Methoden, welche wiederum mit den PyTorch-Funktionen und Methoden arbeiten. Nachfolgend wird exemplarisch die Klasse Conv abgebildet. Darin werden die PyTorchFunktionen Conv2d und BatchNorm2d angewendet.\n",
    "\n",
    "```\n",
    "class Conv(nn.Module):\n",
    "    # Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)\n",
    "    default_act = nn.SiLU()  # default activation\n",
    "\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2)\n",
    "        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def forward_fuse(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "```\n",
    "\n",
    "#### Convolutional Layer\n",
    "\n",
    "YOLOv5 verwendet als ersten Layer einen Convolution Layer mit **[-1, 1, Conv, [64, 6, 2, 2]]**. Der Eintrag ist wie folgt zu lesen.\n",
    "\n",
    "* Reihenfolge im Modell = -1\n",
    "* Anzahl Wiederholungen im Layer = 1\n",
    "* Layer-Art = Conv bzw. Convolutional\n",
    "* Layer-Parameter = [out_channel, kernel_size, stride, padding]\n",
    "\n",
    "Ein Convolutional Layer extrahiert Features aus einem Bild. Das Ergebnis wird Feature Map genannt.\n",
    "\n",
    "Zu Beginn wird das Rohdaten-Bild mit drei Kan√§len (Rot, Gr√ºn und Blau) in das Training √ºbergeben. Die Anzahl der K√§nale von Feature Maps ist wesentlich gr√∂√üer. So sollen bei dem Output des ersten Layers 64 Kan√§le erzeugt werden.\n",
    "\n",
    "Die Gr√∂√üe der Output-Feature Map wird wie folgt berechnet.\n",
    "\n",
    "$ Outputsize = (Inputsize - Kernelsize + 2 * Padding) / Stride + 1 $\n",
    "\n",
    "F√ºr die Layer-Parameter und ein Inputimage mit 640x640 px ergibt sich somit folgende Outputsize.\n",
    "\n",
    "$ 320 = (640 - 6 + 2 * 2) / 2 + 1 $\n",
    "\n",
    "Zus√§tzlich wird die Channel-Anzahl von 64 auf 32 halbiert, weil im S-Modell der width-multiple den Out-channel halbiert.\n",
    "\n",
    "Die FeatureMap hat somit die Paremeter:\n",
    "\n",
    "* 320x320\n",
    "* 32 Channel\n",
    "\n",
    "\n",
    "#### CSP Bottleneck Layer\n",
    "\n",
    "Bottleneck Layer werden dazu verwendet um die wachsende Anzahl an Feature Map Channels zu reduzieren und relavante Feature Maps zu extrahieren.  \n",
    "Die C3 Layer sind \"simplified CSP modules with 3 convolutions {cite:p}`YOLOv5_Issue6930`\", welche das Verh√§ltnis von Input zu Output Channels halbieren.\n",
    "\n",
    "\n",
    "#### SPPF\n",
    "\n",
    "Der Einsatz von Spatial Pyramind Pooling erm√∂glicht unterschiedliche Image-Size f√ºr Input Images, welche zuvor f√ºr den Fully Connected Layer fixiert werden musste.\n",
    "\n",
    "Der SPPF ist ein f√ºr YOLOv5 geschriebenes Modul, welches zu den gleichen Ergebnissen wie SPP kommt. Der Output des SPPF Layers wird als Input f√ºr einen Convolutional Layer verwendet, weshalb die urspr√ºngliche Image Size Fixierung nicht der Einsatzgrund ist. Lt. Issue-Diskussion hei√üt es, dass der SPPF Layer lediglich zu besseren Ergebnissen bei der Detection f√ºhrt. {cite:p}`YOLOv5_Issue4420`\n",
    "\n",
    "\n",
    "#### Upsample\n",
    "\n",
    "Die ist eine reine Funktion von Pytorch. Durch das Upsampling wird die Bildgr√∂√üe, √ºber den scale_factor=2, verdoppelt. Dazu wird der Modus \"nearest\" verwendet.\n",
    "\n",
    "#### Concat\n",
    "\n",
    "Bei der von PyTorch zur Verf√ºgung gestellten Concat-Funktion werden die Tensoren verbunden. In diesem Fall √ºber die Dimension *Channels*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRKapHDQw_z-"
   },
   "source": [
    "## Working Directory\n",
    "\n",
    "F√ºr die Arbeit mit YOLOv5 m√ºssen die python-Dateien √ºber Terminal-Eingaben aufgerufen werden. Daf√ºr wird das Working Directory hier vorgestellt. Dabei wird die Beschreibung auf verwendete Strukturen des YOLOv5 Repository beschr√§nkt.\n",
    "\n",
    "Im Ordner *data* liegen zwei Arten von YAML-Dateien. Die YAML-Datei welche Labels und Bilder zusammenf√ºhrt sowie die YAML-Datei mit Hyperparametern. YOLOv5 verwendet Standarddateien, wenn kein Pfad √ºbergeben wird.\n",
    "\n",
    "Diese YAML-Dateien sind nicht zu verwechseln mit den oben beschriebenen Modell-YAML-Dateien.\n",
    "\n",
    "Im unten dargestellten Working Directory wird die *planes-and-helicopters.yaml* Datei verwendet, um Bilddateien und Labeldateien zusammenzuf√ºhren. Grunds√§tzlich geht YOLOv5 davon aus, dass diese Dateien in einem Ordner auf der Ebene des Repositories abgelegt sind. Diese unterscheiden zwischen Trainings- und Validierungsdaten. Die Labels sind in Textdateien gespeichert und haben den Dateinamen der Bilddatei.\n",
    "\n",
    "Der Ordner *runs* enth√§lt die Ergebnisse aus Training, Validation und Detection. Mit Detection sind die Ergebnisse der Inference (Modellanwendung) gemeint.\n",
    "\n",
    "F√ºr die jeweilige Aufgabe Training, Validation oder Detection gibt es je ein py-File, welches mit der gegebenen Struktur arbeitet. Daher muss mit YOLOv5 innerhalb des Repository gearbeitet werden.\n",
    "\n",
    "```\n",
    "Ablage Bilder und Label\n",
    "|\n",
    "+-- images\n",
    "|   +-- train\n",
    "|   |   |\n",
    "|   |   +-- exampleIMG001.png\n",
    "|   |\n",
    "|   +-- validation\n",
    "|\n",
    "|\n",
    "+-- labels\n",
    "|   +-- train\n",
    "|   |   |\n",
    "|   |   +-- exampleIMG001.txt\n",
    "|   |\n",
    "|   +-- validation  \n",
    "|\n",
    "|\n",
    "yolov5 Repository\n",
    "|\n",
    "+-- data\n",
    "|   +-- hyps\n",
    "|   |   |\n",
    "|   |   +-- hyp.scratch-low.yaml\n",
    "|   |   +-- hyp.VOC.yaml\n",
    "|   |\n",
    "|   +-- planes-and-helicopters.yaml\n",
    "|\n",
    "+-- models\n",
    "|   |\n",
    "|   +-- yolov5n.yaml\n",
    "|   +-- yolov5s.yaml\n",
    "|   +-- yolov5m.yaml\n",
    "|   +-- yolov5l.yaml\n",
    "|   +-- yolov5x.yaml\n",
    "|\n",
    "+-- runs\n",
    "|   +-- detect\n",
    "|   |   |\n",
    "|   |   +-- exp\n",
    "|   |       |\n",
    "|   |       +-- image with detection result\n",
    "|   |\n",
    "|   +-- train\n",
    "|   |   |\n",
    "|   |   +-- exp\n",
    "|   |       |\n",
    "|   |       +--weights\n",
    "|   |       |  |\n",
    "|   |       |  +-- best.pt\n",
    "|   |       |  +-- last.pt\n",
    "|   |       |\n",
    "|   |       +-- image files with metrics, confusion matrix, etc.\n",
    "|   |    \n",
    "|   +-- val\n",
    "|   |   |\n",
    "|   |   +-- exp/run\n",
    "|   |       |\n",
    "|   |       +-- image files with confustion matrix and metrics\n",
    "|\n",
    "+-- detect.py\n",
    "+-- train.py\n",
    "+-- val.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZJOf7RXnRpJYyOVK4T/al",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b8dfdb05e476a0fb0e450349f9b76abd6ac1559882404eeba22108e759c936b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}